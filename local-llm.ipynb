{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install langchain huggingface_hub transformers sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace\n",
    "Hugging Face provides two LLM wrappers: one for local pipelines and another for models hosted on the Hugging Face Hub. These wrappers work with LLMs supporting text-to-text generation and text generation tasks.\n",
    "\n",
    "**Local Pipeline Wrapper**: Simplifies integrating LLMs into your local environment for text generation or text-to-text generation tasks.\n",
    "\n",
    "**Hugging Face Hub Wrapper**: Enables seamless utilization of pre-trained LLMs hosted on the Hugging Face Hub, without local setup or training.\n",
    "\n",
    "Both wrappers offer a consistent interface, abstracting away complexities and allowing you to leverage LLMs efficiently, whether hosted locally or on the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain-huggingface: 0.1.2\n",
    "\n",
    "**chat_models**\n",
    "\n",
    "chat_models.huggingface.ChatHuggingFace\n",
    "\n",
    "Hugging Face LLM's as ChatModels.\n",
    "\n",
    "\n",
    "Message to send to the TextGenInference API.\n",
    "\n",
    "Response from the TextGenInference API.\n",
    "\n",
    "**embeddings**\n",
    "\n",
    "embeddings.huggingface.HuggingFaceEmbeddings\n",
    "\n",
    "HuggingFace sentence_transformers embedding models.\n",
    "\n",
    "embeddings.huggingface_endpoint.HuggingFaceEndpointEmbeddings\n",
    "\n",
    "HuggingFaceHub embedding models.\n",
    "\n",
    "**llms**\n",
    "\n",
    "llms.huggingface_endpoint.HuggingFaceEndpoint\n",
    "\n",
    "HuggingFace Endpoint.\n",
    "\n",
    "llms.huggingface_pipeline.HuggingFacePipeline\n",
    "\n",
    "HuggingFace Pipeline API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Chat Hugging Face\n",
    "\n",
    "\n",
    "Works with HuggingFaceTextGenInference, HuggingFaceEndpoint, HuggingFaceHub, and HuggingFacePipeline LLMs.\n",
    "\n",
    "Upon instantiating this class, the model_id is resolved from the url provided to the LLM, and the appropriate tokenizer is loaded from the HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "result = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'aime programmer.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The FIFA World Cup is an international football tournament. It takes place every four years. In 1994, the tournament was held in the United States of America. The winning team was Brazil. They defeated Italy in the final match, which took place on July 17, 1994. So, the answer is Brazil won the FIFA World Cup in 1994.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    ")\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "print(llm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEmbeddings\n",
    "\n",
    "HuggingFace sentence_transformers embedding models.\n",
    "\n",
    "To use, you should have the sentence_transformers python package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hfe = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = hfe.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.048951808363199234, -0.039862047880887985, -0.021562788635492325]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpointEmbeddings\n",
    "\n",
    "HuggingFaceHub embedding models.\n",
    "\n",
    "To use, you should have the huggingface_hub python package installed, and the environment variable HUGGINGFACEHUB_API_TOKEN set with your API token, or pass it as a named parameter to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "hfee = HuggingFaceEndpointEmbeddings(\n",
    "    model=model,\n",
    "    task=\"feature-extraction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = hfee.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.048951830714941025, -0.03986202925443649, -0.021562786772847176]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gllm=HuggingFaceHub(repo_id=\"google/flan-t5-large\",  model_kwargs={\"temperature\":0,  \"max_length\":100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1994 FIFA World Cup was won by Brazil. Brazil won the 1994 FIFA World Cup. So the answer is Brazil.\n"
     ]
    }
   ],
   "source": [
    "gllm_chain = prompt | gllm | StrOutputParser()\n",
    "print(gllm_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example with answerdotai/ModernBERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = HuggingFaceHub(\n",
    "    repo_id=\"distilbert/distilgpt2\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is inside area 51?\n",
      "\n",
      "Answer: Let's think step by step. The first thing you need to do is look at the map and see if there are any areas that have been identified in the map. If you're looking for a place where you can find your own, then it's important to look at the map and make sure that you don't miss any of the places. You should also be aware that there are no other areas on the map. This is an area that has been identified as the \"home\" of the player. It's not a home. There are only two locations on the map.\n",
      "The second thing you need to do is look at the map and see if there are any areas that have been identified in the map. If you're looking for a place where you can find your own, then it's important to look at the map and make sure that you don't miss any of the places. You should also be aware that there are no other areas on the map. This is an area that has been identified as the \"home\" of the player. It's not a home. There are only two locations on the map.\n",
      "The third thing you need to do is look at the map and see if there are any areas that have been identified in the map. If you're looking for a place where you can find your own, then it's important to look at the map and make sure that you don't miss any of the places. You should also be aware that there are no other areas on the map. This is an area that has been identified as the \"home\" of the player. It's not a home. There are only two locations on the map.\n",
      "The fourth thing you need to do is look at the map and see if there are any areas that have been identified in the map. If you're looking for a place where you can find your own, then it's important to look at the map and make sure that you don't miss any of the places. You should also be aware that there are no other areas on the map. This is an area that has been identified as the \"home\" of the player. It's not a home. There are only two locations on the map.\n",
      "The fifth thing you need to do is look at the map and see if there are any areas that have been identified in the map. If you're looking for a place where you can find your own, then it's important to look at the map and make sure that you don't miss any of the places. You should also be aware\n"
     ]
    }
   ],
   "source": [
    "gptllm_chain = prompt | model\n",
    "question = \"What is inside area 51?\"\n",
    "\n",
    "print(gptllm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download models locally.\n",
    "You can download and use any open-source, unrestricted Hugging Face model if you have sufficient VRAM and storage. There are two main approaches:\n",
    "\n",
    "- Pipeline Approach \n",
    "- Hugging Face Auto Classes\n",
    "\n",
    "The choice depends on your requirements, resources, and the level of customization needed.\n",
    "\n",
    "**Pipeline Option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:3\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\" , device=3,   max_length=100)\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Egypt?\n",
      "\n",
      "Answer: Let's think step by step.\n",
      "A few things. The state of Libya has made an extraordinary contribution, because Egypt's people are still struggling in financial crisis and political upheaval, because they are not very happy in their own sense of self, or at all.\n",
      "So, the question is how do we explain its state collapse, or how can we explain it when we put together some solutions?\n",
      "One way is, Egypt's\n"
     ]
    }
   ],
   "source": [
    "gptllmhf_chain = prompt | local_llm\n",
    "question = \"What is the capital of Egypt?\"\n",
    "\n",
    "print(gptllmhf_chain.invoke(question, truncation = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auto Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:3\n",
      "/home/himanshulalarya/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cairo\n",
      "\n",
      "Direct model output: cairo\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# 1. Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# 2. Create a pipeline\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    temperature=0,\n",
    "    device = 3,\n",
    ")\n",
    "\n",
    "# 3. Create a HuggingFacePipeline instance\n",
    "acllm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 4. Create a prompt template\n",
    "prompt = PromptTemplate.from_template(\"{question}\")\n",
    "\n",
    "# 5. Create the chain\n",
    "chain = prompt | acllm\n",
    "\n",
    "# 6. Run the chain\n",
    "question = \"What is the capital of Egypt?\"\n",
    "result = chain.invoke({\"question\": question})\n",
    "print(result)\n",
    "\n",
    "# If you want to use the model directly without LangChain:\n",
    "input_ids = tokenizer(question, return_tensors=\"pt\").input_ids\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "direct_result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nDirect model output:\", direct_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshulalarya/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/himanshulalarya/miniconda3/envs/test/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:3\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm_hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"google/flan-t5-large\",\n",
    "    task=\"text2text-generation\",\n",
    "    device = 3,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 256}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Egypt is Cairo. Cairo is the capital of Egypt. So the answer is Cairo.\n"
     ]
    }
   ],
   "source": [
    "llm_hf_chain = prompt | llm_hf\n",
    "question = \"What is the capital of Egypt?\"\n",
    "\n",
    "print(llm_hf_chain.invoke(question, truncation = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot usecase\n",
    "Few Shot Learning Contextual Chatbot: This example demonstrates the simplest way conversational context can be managed within a LLM based chatbot.\n",
    "\n",
    "Longer conversations can be solved for in two ways:\n",
    "- Truncating the conversational history, hence removing the first portion of the conversation history at set stages. This approach is analogous to limiting log files to a certain size via rolling logs.\n",
    "\n",
    "- The second approach is making use of LLMs to summarise the conversation history, as the conversations continue.\n",
    "\n",
    "Within LangChain ConversationBufferMemory can be used as type of memory that collates all the previous input and output text and add it to the context passed with each dialog sent from the user.\n",
    "\n",
    "Reference: https://cobusgreyling.medium.com/langchain-creating-large-language-model-llm-applications-via-huggingface-192423883a74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3828814/3992475228.py:7: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory=ConversationBufferMemory()\n",
      "/tmp/ipykernel_3828814/3992475228.py:4: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What is the capital of Egypt?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cairo is the capital of Egypt.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm_hf, \n",
    "    verbose=True, \n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "conversation.predict(input=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
